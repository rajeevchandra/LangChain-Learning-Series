{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d783788c",
      "metadata": {},
      "source": [
        "# Day 01 â€” Intro to LangChain\n",
        "\n",
        "This notebook auto-routes to Ollama if `OLLAMA_BASE_URL` is set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0b92c52",
      "metadata": {},
      "source": [
        "## Configuration (`config.py`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "13eba145",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
        "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"\")\n",
        "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
        "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"llama3.2:1b\")\n",
        "\n",
        "def get_openai_or_ollama_client():\n",
        "    \"\"\"Return (provider, client, model_name).\"\"\"\n",
        "    from openai import OpenAI\n",
        "    if OLLAMA_BASE_URL:\n",
        "        return \"ollama\", OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\"), OLLAMA_MODEL\n",
        "    else:\n",
        "        return \"openai\", OpenAI(api_key=OPENAI_API_KEY), OPENAI_MODEL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "169fcab7",
      "metadata": {},
      "source": [
        "## Example code (`main.py`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "50021929",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, it's nice to meet you.\n"
          ]
        }
      ],
      "source": [
        "from config import get_openai_or_ollama_client, OPENAI_API_KEY, OPENAI_MODEL\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "def main():\n",
        "    provider, client, model = get_openai_or_ollama_client()\n",
        "    if provider == \"ollama\":\n",
        "        # Use OpenAI SDK against Ollama's OpenAI-compatible endpoint\n",
        "        resp = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": \"Say hello in one big sentence.\"}],\n",
        "            temperature=0.2,\n",
        "        )\n",
        "        print(resp.choices[0].message.content)\n",
        "    else:\n",
        "        # Use LangChain's ChatOpenAI for OpenAI proper\n",
        "        llm = ChatOpenAI(api_key=OPENAI_API_KEY, model=OPENAI_MODEL)\n",
        "        resp = llm.invoke([HumanMessage(content=\"Say hello in one short sentence.\")])\n",
        "        print(resp.content)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
